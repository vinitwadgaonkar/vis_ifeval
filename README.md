# vis_ifeval

Visual Instruction Following Evaluation Benchmark

## Overview

`vis_ifeval` is a Python benchmark for evaluating visual instruction following capabilities of image generation models. The benchmark tests whether generated images satisfy dense prompts with multiple constraints (8-12 per prompt).

### Key Concepts

- **Prompts**: Dense text descriptions with multiple constraints
- **Constraints**: Specific requirements that must be satisfied (e.g., object counts, text content, spatial relationships)
- **VIPR (Visual Instruction Pass Rate)**: Primary metric measuring the percentage of constraints satisfied

## Current Status

### Implemented

- âœ… Dummy image model (generates random images for testing)
- âœ… SDXL model hook (requires torch and diffusers)
- âœ… Text evaluator using Tesseract OCR with backend abstraction
- âœ… Nutrition label evaluator (table_slot constraints) with OCR parsing
- âœ… Logic evaluator (percent_dv_consistency for sodium)
- âœ… Negative evaluator (placeholder with CLIP hook ready)
- âœ… OCR backend abstraction (Tesseract + placeholder for advanced backends)
- âœ… End-to-end pipeline (generate â†’ evaluate â†’ aggregate)
- âœ… Weights & Biases integration for experiment tracking

### Partially Implemented / Stubs

- ðŸ”² Composition evaluator (count, attribute, spatial, state constraints) - stub
- ðŸ”² CLIP-based negative evaluator (placeholder ready, needs CLIP integration)
- ðŸ”² Advanced OCR backends (Surya, DeepSeek-OCR) - placeholder ready

## Installation

1. Install dependencies:

```bash
pip install -r requirements.txt
```

2. Install Tesseract OCR (required for text evaluation):

**macOS:**
```bash
brew install tesseract
```

**Ubuntu/Debian:**
```bash
sudo apt-get install tesseract-ocr
```

**Windows:**
Download from [GitHub](https://github.com/UB-Mannheim/tesseract/wiki)

## Usage

### Basic Pipeline

Run the full evaluation pipeline:

```bash
# 1. Generate images
python -m vis_ifeval.runners.generate_images

# 2. Evaluate constraints
python -m vis_ifeval.runners.evaluate_constraints

# 3. Aggregate metrics
python -m vis_ifeval.runners.aggregate_metrics
```

### Command Line Options

All runners support `--model-name` and `--use-wandb` flags:

```bash
# Generate with custom model
python -m vis_ifeval.runners.generate_images --model-name dummy

# Enable W&B logging
python -m vis_ifeval.runners.generate_images --use-wandb

# Evaluate with W&B
python -m vis_ifeval.runners.evaluate_constraints --model-name dummy --use-wandb
```

### Weights & Biases Integration

Enable W&B logging to track experiments and visualize results:

1. **Set environment variables:**

```bash
export VIS_IFEVAL_USE_WANDB=1
export VIS_IFEVAL_WANDB_PROJECT=vis-ifeval
export VIS_IFEVAL_WANDB_ENTITY=your-username  # optional
export VIS_IFEVAL_WANDB_GROUP=experiment-name  # optional
```

2. **Or use command line flags:**

```bash
python -m vis_ifeval.runners.generate_images --use-wandb
```

3. **What gets logged:**

   - **Generation step**: Per-image latency, model name, category, prompt ID, sample images
   - **Evaluation step**: Per-constraint scores and labels, constraint types, sample images with scores
   - **Aggregation step**: VIPR metrics, VIPR by type, VIPR by category, latency statistics

4. **Available dashboards:**

   - VIPR by constraint type (bar chart)
   - VIPR by category (bar chart)
   - Latency distribution (histogram)
   - Score distributions by constraint type (histograms/boxplots)
   - Sample image gallery with captions

The system gracefully degrades if wandb is not installed or no API key is presentâ€”it will print a warning and continue without logging.

## Project Structure

```
vis_ifeval/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ vis_ifeval/
â”‚       â”œâ”€â”€ config.py              # Configuration management
â”‚       â”œâ”€â”€ models/                # Image generation models
â”‚       â”‚   â”œâ”€â”€ base_model.py
â”‚       â”‚   â”œâ”€â”€ dummy_model.py
â”‚       â”‚   â””â”€â”€ sdxl_model.py     # SDXL model (optional)
â”‚       â”œâ”€â”€ evaluators/            # Constraint evaluators
â”‚       â”‚   â”œâ”€â”€ base.py
â”‚       â”‚   â”œâ”€â”€ text_eval.py       # Text evaluator (OCR-based)
â”‚       â”‚   â”œâ”€â”€ label_eval.py      # Nutrition label evaluator
â”‚       â”‚   â”œâ”€â”€ logic_eval.py      # Logic consistency evaluator
â”‚       â”‚   â”œâ”€â”€ negative_eval.py   # Negative constraint evaluator (CLIP placeholder)
â”‚       â”‚   â””â”€â”€ comp_eval.py       # Composition evaluator (stub)
â”‚       â”œâ”€â”€ runners/               # Pipeline scripts
â”‚       â”‚   â”œâ”€â”€ generate_images.py
â”‚       â”‚   â”œâ”€â”€ evaluate_constraints.py
â”‚       â”‚   â””â”€â”€ aggregate_metrics.py
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ io.py
â”‚           â”œâ”€â”€ ocr_backend.py     # OCR backend abstraction
â”‚           â”œâ”€â”€ clip_utils.py      # CLIP utilities (placeholder)
â”‚           â””â”€â”€ wandb_logger.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ prompts.jsonl             # Benchmark prompts
â”œâ”€â”€ data/
â”‚   â””â”€â”€ outputs/                  # Generated images
â”œâ”€â”€ results/                      # Evaluation results
â”‚   â”œâ”€â”€ generation_*.jsonl
â”‚   â”œâ”€â”€ scores_*.jsonl
â”‚   â””â”€â”€ metrics_*.json
â””â”€â”€ requirements.txt
```

## Prompt Format

Prompts are stored in JSONL format (`prompts/prompts.jsonl`). Each line is a JSON object:

```json
{
  "id": "comp_001",
  "category": "composition",
  "prompt": "A photo of three blue mugs...",
  "constraints": [
    {
      "id": "mug_count",
      "type": "count",
      "object": "blue mug",
      "target": 3
    },
    ...
  ]
}
```

### Constraint Types

- `text`: Text content (evaluated with OCR) âœ…
- `table_slot`: Nutrition label fields (evaluated with OCR parsing) âœ…
- `logic`: Logical relationships (e.g., percent_dv_consistency) âœ…
- `negative`: Absence checks (placeholder, CLIP-ready) ðŸ”²
- `count`: Object counts (stub) ðŸ”²
- `attribute`: Object attributes (stub) ðŸ”²
- `spatial`: Spatial relationships (stub) ðŸ”²
- `state`: Object states (stub) ðŸ”²

## Extending the Benchmark

### Adding a New Model

1. Create a new model class in `src/vis_ifeval/models/` that inherits from `ImageModel`
2. Implement the `generate()` method
3. Update `_build_model()` in `generate_images.py` to add your model

Example:
```python
elif model_name == "my_model":
    from vis_ifeval.models.my_model import MyModel
    return MyModel()
```

### Adding a New Evaluator

1. Create a new evaluator class in `src/vis_ifeval/evaluators/` that inherits from `ConstraintEvaluator`
2. Implement `can_handle()` and `score()` methods
3. Register it in `EvaluatorRegistry` (in `evaluators/__init__.py`)

If your evaluator needs OCR, accept a `TextBackend` in the constructor and use it for text extraction.

## License

[Add your license here]

